{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 导入numpy \n",
    "import numpy as np\n",
    "# 导入word2vec 文字转向量包\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# 导入jieba分词\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本读取数据\n",
    "def load_data(file):\n",
    "    data = open(file)\n",
    "    return data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_data('data3.txt')\n",
    "# 处理文本数据分离文本和标签\n",
    "X=[]\n",
    "Y_=[]\n",
    "for i in data:\n",
    "    text = i.split('***',1)\n",
    "    if len(text) == 2:\n",
    "        X.append(text[0]) \n",
    "        Y_.append(text[1].replace('\\n',''))\n",
    "Y_value = list(set(Y_))\n",
    "class_data = {i:Y_value.index(i) for i in Y_value}\n",
    "Y_data = [class_data[i] for i in Y_]\n",
    "# 转为onehot编码\n",
    "Y = to_categorical(np.array(Y_data))\n",
    "print(Y)\n",
    "# 处理文本\n",
    "sentences_list = []\n",
    "for line in X:\n",
    "    single_list = line.strip().split(' ')\n",
    "    single_list = jieba.analyse.extract_tags(single_list[0],topK=20,withWeight=False,allowPOS=())\n",
    "    while '' in single_list:\n",
    "        single_list.remove('')\n",
    "    sentences_list.append(single_list)\n",
    "print(sentences_list)\n",
    "with open('class_data.txt','w') as f:\n",
    "    f.write(str(class_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用word2vec 模型返回字典和词的向量\n",
    "\n",
    "def create_dictionaries(model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(),allow_update=True)\n",
    "    w2indx = {v:k+1 for k,v in gensim_dict.items()}\n",
    "    w2vec = {word:model[word] for word in w2indx.keys()}\n",
    "    return w2indx,w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xms/.virtualenvs/dl/lib/python3.5/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# word2vec 方法转向量\n",
    "model = Word2Vec(sentences_list,size=100,min_count=5,window=5)\n",
    "index_dict,word_vectors = create_dictionaries(model)\n",
    "\n",
    "with open('index_dict.txt','w') as f:\n",
    "    f.write(str(index_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13745, 100)\n"
     ]
    }
   ],
   "source": [
    "# 生成词嵌入向量\n",
    "# 把上边一个词向量的长度转为100\n",
    "n_symbols = len(index_dict) +1\n",
    "embedding_weights = np.zeros((n_symbols,100))\n",
    "for w,index in index_dict.items():\n",
    "    embedding_weights[index,:] = word_vectors[w]\n",
    "print(embedding_weights.shape)\n",
    "# 词有7767个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 484, 9854, 3471, 2577, 3804, 6853, 4996, 3005, 10648]\n"
     ]
    }
   ],
   "source": [
    "# 在字典中找到词返回索引\n",
    "# 文本和词典匹配将我们的词特征转为数字\n",
    "def text_to_index_array(dic,sentence):\n",
    "    new_sentence = []\n",
    "    for sen in sentence:\n",
    "        new_sen = []\n",
    "        for word in sen:\n",
    "            try:\n",
    "                new_sen.append(dic[word])\n",
    "            except:\n",
    "                new_sen.append(0)\n",
    "        new_sentence.append(new_sen)\n",
    "    return np.array(new_sentence)\n",
    "\n",
    "x = text_to_index_array(index_dict,sentences_list)\n",
    "print(x[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense,Dropout,Activation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88053, 50)\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集生成训练和测试4:1\n",
    "# sklearn包划分数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,Y,test_size=0.2)\n",
    "# 把x输入特征标准化，不够的补0是每一个输入的x 为50长度\n",
    "x_train = sequence.pad_sequences(x_train,maxlen=50)\n",
    "x_test = sequence.pad_sequences(x_test,maxlen=50)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xms/.virtualenvs/dl/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(recurrent_activation=\"hard_sigmoid\", units=50, activation=\"relu\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 输入特征\n",
    "model.add(Embedding(output_dim=100,input_dim=n_symbols,mask_zero=True,weights=[embedding_weights]))\n",
    "# model.add(Dense(12))\n",
    "# lstm隐藏\n",
    "model.add(LSTM(output_dim=50,activation='relu',inner_activation='hard_sigmoid'))\n",
    "# 随机失活\n",
    "model.add(Dropout(0.5))\n",
    "# 全连接层 输出12个分类\n",
    "model.add(Dense(12))\n",
    "# softmax激活\n",
    "model.add(Activation('softmax'))\n",
    "# 多分类损失函数，梯度下降\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88053 samples, validate on 22014 samples\n",
      "Epoch 1/20\n",
      "88053/88053 [==============================] - 73s 830us/step - loss: 1.4605 - acc: 0.5214 - val_loss: 1.0045 - val_acc: 0.6799\n",
      "Epoch 2/20\n",
      "88053/88053 [==============================] - 73s 827us/step - loss: 0.9574 - acc: 0.7042 - val_loss: 0.7743 - val_acc: 0.7672\n",
      "Epoch 3/20\n",
      "88053/88053 [==============================] - 73s 827us/step - loss: 0.7403 - acc: 0.7783 - val_loss: 0.7047 - val_acc: 0.7833\n",
      "Epoch 4/20\n",
      "88053/88053 [==============================] - 74s 835us/step - loss: 0.6246 - acc: 0.8162 - val_loss: 0.6830 - val_acc: 0.7971\n",
      "Epoch 5/20\n",
      "88053/88053 [==============================] - 74s 836us/step - loss: 0.5514 - acc: 0.8372 - val_loss: 0.7035 - val_acc: 0.7993\n",
      "Epoch 6/20\n",
      "88053/88053 [==============================] - 74s 835us/step - loss: 0.4935 - acc: 0.8536 - val_loss: 0.7319 - val_acc: 0.8044\n",
      "Epoch 7/20\n",
      "88053/88053 [==============================] - 74s 837us/step - loss: 0.4504 - acc: 0.8664 - val_loss: 0.7807 - val_acc: 0.8038\n",
      "Epoch 8/20\n",
      "88053/88053 [==============================] - 74s 836us/step - loss: 0.4167 - acc: 0.8771 - val_loss: 0.7547 - val_acc: 0.8096\n",
      "Epoch 9/20\n",
      "88053/88053 [==============================] - 74s 838us/step - loss: 0.3849 - acc: 0.8855 - val_loss: 0.7785 - val_acc: 0.8109\n",
      "Epoch 10/20\n",
      "88053/88053 [==============================] - 74s 838us/step - loss: 0.3582 - acc: 0.8930 - val_loss: 0.8548 - val_acc: 0.8079\n",
      "Epoch 11/20\n",
      "88053/88053 [==============================] - 74s 845us/step - loss: 0.3354 - acc: 0.8992 - val_loss: 0.9061 - val_acc: 0.8060\n",
      "Epoch 12/20\n",
      "88053/88053 [==============================] - 75s 847us/step - loss: 0.3166 - acc: 0.9042 - val_loss: 0.9200 - val_acc: 0.8105\n",
      "Epoch 13/20\n",
      "88053/88053 [==============================] - 75s 852us/step - loss: 0.3025 - acc: 0.9088 - val_loss: 0.9223 - val_acc: 0.8114\n",
      "Epoch 14/20\n",
      "88053/88053 [==============================] - 75s 851us/step - loss: 0.2831 - acc: 0.9145 - val_loss: 0.9712 - val_acc: 0.8136\n",
      "Epoch 15/20\n",
      "88053/88053 [==============================] - 75s 853us/step - loss: 0.2684 - acc: 0.9175 - val_loss: 1.0650 - val_acc: 0.8113\n",
      "Epoch 16/20\n",
      "88053/88053 [==============================] - 75s 852us/step - loss: 0.2569 - acc: 0.9198 - val_loss: 1.0634 - val_acc: 0.8131\n",
      "Epoch 17/20\n",
      "88053/88053 [==============================] - 75s 849us/step - loss: 0.2477 - acc: 0.9245 - val_loss: 1.0640 - val_acc: 0.8140\n",
      "Epoch 18/20\n",
      "88053/88053 [==============================] - 75s 849us/step - loss: 0.2368 - acc: 0.9272 - val_loss: 1.1200 - val_acc: 0.8141\n",
      "Epoch 19/20\n",
      "88053/88053 [==============================] - 75s 852us/step - loss: 0.2205 - acc: 0.9307 - val_loss: 1.2419 - val_acc: 0.8118\n",
      "Epoch 20/20\n",
      "88053/88053 [==============================] - 75s 855us/step - loss: 0.2178 - acc: 0.9329 - val_loss: 1.1842 - val_acc: 0.8138\n",
      "22014/22014 [==============================] - 6s 263us/step\n",
      "1.184155183266454 0.8137548831748824\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 100)         1374500   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 12)                612       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 1,405,312\n",
      "Trainable params: 1,405,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=128,epochs=20,validation_data=(x_test,y_test))\n",
    "score,acc = model.evaluate(x_test,y_test,batch_size=128)\n",
    "print(score,acc)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "this_model = load_model('weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通知提醒\n"
     ]
    }
   ],
   "source": [
    "# 直接输入一句话对其进行预测\n",
    "def convert_vector_predict(str_r):\n",
    "    new_str = jieba.analyse.extract_tags(str_r,topK=20,withWeight=False,allowPOS=())\n",
    "#     print(new_str)\n",
    "    x = text_to_index_array(index_dict,[new_str])\n",
    "    x = sequence.pad_sequences(x,maxlen=50)\n",
    "#     print(x)\n",
    "    y = this_model.predict_classes(x)\n",
    "    return y\n",
    "value = convert_vector_predict('孩子出不出色，关注在于这个……')\n",
    "print([k for k,v in class_data.items() if v==value[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3(dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
